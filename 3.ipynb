{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0969754-340b-4ddd-ad9a-1579a6f7f1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lower the toxicity using a hate speech reward model, rewarding for low hate\n",
    "# Using PPO and quantitative and qualitative evaluation\n",
    "# trl will give access to PPO trainer\n",
    "!pip install torch torchdata transformers datasets evaluate rouge_score peft trl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219a4010-8191-4fb6-a148-7422cc28358b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AutoModelForSequenceClassification -- load our facebook sequence classifier // if hate speech or not\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# transformers reinforcement learning - trl\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead #specific need for PPO\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler #filter / sample from our data / first 5/12\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#tdqm lib makes the oops show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bada8744-fb1a-4efe-a80c-bb8fdb17dbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/natalr2/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4939cc1c6d64bae83b05b3602b080e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name= 'google/flan-t5-base'\n",
    "ds_hf = \"knkarthick/dialogsum\"\n",
    "\n",
    "original_ds = load_dataset(ds_hf)\n",
    "\n",
    "original_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c11527fd-411a-4338-abc8-49586dc5d992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/natalr2/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at C:\\Users\\natalr2\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-cd36827d3490488d\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d904df43a047dc8b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length,\n",
    "                  input_max_text_length):\n",
    "    \n",
    "    #load dataset, in this case only train will be enough\n",
    "    dataset = load_dataset(dataset_name, split = \"train\")\n",
    "    \n",
    "    #filter the dialogues of len between min and max\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length)\n",
    "    \n",
    "    #prepare tokenize, device_map auto allows to switch between GPU and CPU automatically\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        \n",
    "        # wrap dialog with instruction\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        \n",
    "        # this must be called \"query\", which is a requirement of PPO lib\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "        \n",
    "    \n",
    "    #tokenize each dialogue\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type = \"torch\")\n",
    "    \n",
    "    dataset_splits = dataset.train_test_split(test_size = 0.2, shuffle = False, seed = 42)\n",
    "    \n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=ds_hf,\n",
    "                        input_min_text_length= 200,\n",
    "                        input_max_text_length = 1000 \n",
    "                        \n",
    "                       )\n",
    "\n",
    "print(dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a399e4b4-a32b-4446-bc78-8bd2274c2ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params} \\npercentage of trainable model parameters: ({all_model_params}/{trainable_model_params})\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c562afce-db0f-409f-a79c-8a81f708b2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PEFT model parameters to be updated:\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800 \n",
      "percentage of trainable model parameters: (251116800/3538944)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, #rank -- high\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       \"./peft-dialogue-summary-checkpoint-local/\", \n",
    "                                        lora_config=lora_config,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        device_map=\"auto\",\n",
    "                                        is_trainable=True)\n",
    "\n",
    "\n",
    "print(f' PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "535a5b1b-74f2-4d32-b9db-44b3cd3fe4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PPO model parameters to be updated (ValueHead + 769 params):\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569 \n",
      "percentage of trainable model parameters: (251117569/3539713)\n",
      "\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# preparing the model to fine-tune LLM using RL\n",
    "#prepare PPO model, passing the PEFT model to ir, PPO will be used to optimze the RL policy against the reward model\n",
    "\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model, \n",
    "                                                                torch_dtype=torch.bfloat16,\n",
    "                                                              is_trainable=True) # putting the model in fine tuning mode // # to generate summaries/predictions we set it to false\n",
    "\n",
    "print(f' PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa44d34-85b2-4466-9b86-6972ff6c48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3539713 = 769 parameter more than the last model = 768 of the ValueHead + 1 that is our BIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "151ea7b2-0579-4c50-aae2-3e166b808775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameteres to be updated\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569 \n",
      "percentage of trainable model parameters: (251117569/0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "# kl divergence use to compare the original model to the ppo model\n",
    "print(f'Reference model parameteres to be updated\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac5f21d-b5e1-46b8-b30d-b981214ee86a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8dcd6b139d4839a4ec0d372e8ff8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natalr2\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\natalr2\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a07ec550b8646ed8c42564b64b158c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc29679b7df34a7ba26c4faa81261c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cd130bf86444a896e34579c9116c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baccebc967040708fbd46847274ce17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c804768b03db47d38e55e2fd1e6c407c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "#loading the toxicity model from facebook to identify hate speech\n",
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r2-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\") #classifier with two labels, not hate and hate\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca9cba0a-e8a1-495c-902e-f6e470fe1982",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [nothate, hate]:[4.057403087615967, -3.959798812866211]\n",
      "prob [nothate, hate]:[0.9996703863143921, 0.0003296325448900461]\n",
      "reward [high]:[4.057403087615967]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"I want to kiss you\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [nothate, hate]:{logits.tolist()[0]}')\n",
    "\n",
    "#prob not hate\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'prob [nothate, hate]:{probabilities}')\n",
    "\n",
    "#get logits not hate - THIS IS THE REWARD\n",
    "not_hate_index = 0 # in a lot of cases the 'positive' class could be the oposite\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward [high]:{nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6adae40b-924d-45a3-a3ee-b3e7922747bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [nothate, hate]:[-0.6169748902320862, 0.4805370569229126]\n",
      "prob [nothate, hate]:[0.2502063512802124, 0.7497936487197876]\n",
      "reward [low]:[-0.6169748902320862]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"You are disgusting and terrible and fuck you\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [nothate, hate]:{logits.tolist()[0]}')\n",
    "\n",
    "#prob not hate\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'prob [nothate, hate]:{probabilities}')\n",
    "\n",
    "#get logits not hate - THIS IS THE REWARD\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward [low]:{nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43749da2-3620-418e-bd4a-dc7de87ba7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output for non-toxic text:\n",
      "[{'label': 'nothate', 'score': 4.057403087615967}, {'label': 'hate', 'score': -3.959798812866211}]\n",
      "[{'label': 'nothate', 'score': 0.9996703863143921}, {'label': 'hate', 'score': 0.00032963251578621566}]\n",
      "Reward model output for toxic text:\n",
      "[{'label': 'hate', 'score': 0.4805370569229126}, {'label': 'nothate', 'score': -0.6169748902320862}]\n",
      "[{'label': 'hate', 'score': 0.7497936487197876}, {'label': 'nothate', 'score': 0.2502063512802124}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
    "                          model=toxicity_model_name,\n",
    "                          device=device)\n",
    "\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, #return all scores\n",
    "    \"function_to_apply\": \"none\", # set to none to retrieve raw logits\n",
    "    \"batch_size\": 16 \n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, #return all scores\n",
    "    \"function_to_apply\": \"Softmax\", # set to none to retrieve raw logits\n",
    "    \"batch_size\": 16 \n",
    "}\n",
    "\n",
    "\n",
    "print(\"Reward model output for non-toxic text:\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"Reward model output for toxic text:\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810040f9-dbf4-46ad-a60c-e48b4a6d4efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b385996faafd43a1baaf40927dbc6ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                   toxicity_model_name,\n",
    "                                   module_type = \"measurement\",\n",
    "                                   toxic_label = \"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20ebd452-3e77-4570-87d4-c8307bd3dd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.00032963251578621566]\n",
      "Toxicity score for toxic text:\n",
      "[0.7497936487197876]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[non_toxic_text])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[toxic_text])\n",
    "\n",
    "print(\"Toxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ad075af-2a4d-437f-ae30-52669adaa027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining a function to evaluate and calculate the toxicity scores, means, stddev to further try to reduce toxicity\n",
    "\n",
    "\n",
    "def evaluate_toxicity(model,\n",
    "                      toxicity_evaluator,\n",
    "                      tokenizer,\n",
    "                      dataset,\n",
    "                      num_samples):\n",
    "    \n",
    "    max_new_tokens = 100\n",
    "    \n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    \n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text =  sample['query']\n",
    "        \n",
    "        if i > num_samples:\n",
    "            break\n",
    "            \n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
    "            \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                                 top_k=0.0,\n",
    "                                                 top_p=1.0,\n",
    "                                                 do_sample=True)\n",
    "            \n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                                generation_config=generation_config)\n",
    "            \n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "            \n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "            \n",
    "        \n",
    "\n",
    "    #compute mean and std np\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "925952f8-8d76-42aa-8683-2d19bf800398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:26,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " toxicity [mean, std] before detox: [0.00034751372401263904, 0.00013464479365913763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the goal is to reduce the mean toxicity score\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map = \"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model,\n",
    "                                                                          toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                          tokenizer=tokenizer,\n",
    "                                                                          dataset=dataset[\"test\"],\n",
    "                                                                          num_samples=10)\n",
    "\n",
    "print(f' toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c607556e-94f4-49bd-a689-4fdfee691bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inicitalize PPOT trainer\n",
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "#uncomment the lines to test collator\n",
    "# test_data = [{\"key1\" : \"value1\", \"key2\":\"value2\", \"key3\":\"valuee3\"}]\n",
    "#print(f'Collator input: {test_data}')\n",
    "#print(f'Collator output: {collator(test_data)}')\n",
    "\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config,\n",
    "                         model=ppo_model,\n",
    "                         ref_model=ref_model, # reference to KLdivergence\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=dataset[\"train\"],\n",
    "                         data_collator=collator)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4eabf0cf-b04b-4267-ac81-2d34200311fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [04:01, 241.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.039288848638534546\n",
      "ppo/returns/mean/: 1.8147528171539307\n",
      "ppo/policy/advantages_mean: 0.3819125294685364\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [08:17, 250.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.00917330663651228\n",
      "ppo/returns/mean/: 1.2585248947143555\n",
      "ppo/policy/advantages_mean: 0.26183825731277466\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [12:46, 258.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.03868161141872406\n",
      "ppo/returns/mean/: 1.5983028411865234\n",
      "ppo/policy/advantages_mean: 0.20214909315109253\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [16:28, 244.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.028587106615304947\n",
      "ppo/returns/mean/: 1.6518182754516602\n",
      "ppo/policy/advantages_mean: 0.3893478810787201\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [20:13, 237.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.028780454769730568\n",
      "ppo/returns/mean/: 1.3318476676940918\n",
      "ppo/policy/advantages_mean: 0.2121773362159729\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [23:59, 233.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.03412031754851341\n",
      "ppo/returns/mean/: 1.452364206314087\n",
      "ppo/policy/advantages_mean: 0.19378435611724854\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [26:45, 211.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.023303095251321793\n",
      "ppo/returns/mean/: 2.1469554901123047\n",
      "ppo/policy/advantages_mean: 0.039320722222328186\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [29:40, 199.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.004564889706671238\n",
      "ppo/returns/mean/: 2.022526502609253\n",
      "ppo/policy/advantages_mean: 0.147976815700531\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [32:43, 194.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: -0.007140921428799629\n",
      "ppo/returns/mean/: 1.6228461265563965\n",
      "ppo/policy/advantages_mean: 0.0345914289355278\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [35:59, 215.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 0.0017532026395201683\n",
      "ppo/returns/mean/: 1.8552656173706055\n",
      "ppo/policy/advantages_mean: 0.10104025900363922\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we don't want the KL divergence goes to high, in this case KLdiv trying to hacking\n",
    "\n",
    "# in this code we are grabing each of the samples \n",
    "\n",
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\":1.0,\n",
    "    \"do_sample\":True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, #return all scores\n",
    "    \"function_to_apply\":\"none\", # want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "# summarizing the text\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    #break when you reach max_steps\n",
    "    if step>= max_ppo_steps:\n",
    "        break\n",
    "        \n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "        \n",
    "        # get response from flan-t5/peft llm\n",
    "    summary_tensors = []\n",
    "        \n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "            \n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "            \n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "            \n",
    "    # this needs to be called response\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "  \n",
    "    # using the sentiment pipeline / zipping query and response together and passing this pair to the pipeline / hate or not? / pulling out the response to our ppoTrainer\n",
    "    #executing PPO training to minimize loss\n",
    "    #not actually modifying all the model, just the 1,4% lora parameters that we can change\n",
    "    #compute reward outputs\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "        \n",
    "    #you use nothate item because this is the score for the positive nothate class\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
    "        \n",
    "    #run PPO step\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean/: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')        \n",
    "    print('-'.join('' for x in range(100)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53994c58-1c30-4920-9a85-006fc2932491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:46<00:00,  5.32s/it]\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model quantitativally\n",
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "#get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids = torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "    \n",
    "    summary = ppo_model.generate(\n",
    "        input_ids = torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "    \n",
    "# Decode responses\n",
    "compare_results[\"response_before\"] =  [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] =  [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "#sentiment analysis of query/response pairs before/after\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "801dfed0-fde5-404e-8773-6fb894c9bd20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n",
       "      <td>&lt;pad&gt; Commute your cell phone to your home.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1#: I need to buy some broadband latex. #Person2#: That's right&lt;/s&gt;</td>\n",
       "      <td>3.031390</td>\n",
       "      <td>3.031390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy, please believe everyone. Richard was fired by our manager.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy and Classy about Judy's move.&lt;/s&gt;</td>\n",
       "      <td>3.904954</td>\n",
       "      <td>3.904954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n",
       "      <td>&lt;pad&gt; Dallas C. Park's clinic is located in China's Town Center shopping mall. taxi (town limo) service is now provided, but you can make a local day-trip from Guangzhou.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; and look. and sign in. and leave the prescription.&lt;/s&gt;</td>\n",
       "      <td>3.943122</td>\n",
       "      <td>3.943122</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n",
       "      <td>&lt;pad&gt; To let you know what the express is like.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Assuming food quality was poor and the service wasn't good person2 says... this is a new restaurant.&lt;/s&gt;</td>\n",
       "      <td>3.387951</td>\n",
       "      <td>3.387951</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; People at the Maa * change buttons to cm movement in 10n.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The bill of 150 yuan a person is 1500 dollars.&lt;/s&gt;</td>\n",
       "      <td>3.958113</td>\n",
       "      <td>3.958113</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Talk to Device dealer&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Pict for $10 and saved at person1.&lt;/s&gt;</td>\n",
       "      <td>3.289397</td>\n",
       "      <td>3.289397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n",
       "      <td>&lt;pad&gt; #Person1#: See the final draft of our contract (#1371).&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; ---- My pleasure, Mr. Keith.&lt;/s&gt;</td>\n",
       "      <td>4.364748</td>\n",
       "      <td>4.364748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n",
       "      <td>&lt;pad&gt; Find jobs in the job center.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The characters are asked to pick a job. The job centers have hundreds of job job postings, all right?&lt;/s&gt;</td>\n",
       "      <td>3.839757</td>\n",
       "      <td>3.839757</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n",
       "      <td>&lt;pad&gt; #Person1: Have you talked to your girlfriend this morning? She said, \"I am having trouble with alcoholamps.\" He is then annoyed for smoking in public. He says there are other ways, like nicotine patches or nicotine chewing gum. He is simply sorting through the smoke and this lady became nauseated. #Person1: Sorry honey too, honey. You're still smoking!!&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The two people are talking about approval.&lt;/s&gt;</td>\n",
       "      <td>2.772885</td>\n",
       "      <td>2.772885</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Person1 needs a check to make sure she prepared good, original papers. They should be translated well into English.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Hopefully it will be perfect soon.&lt;/s&gt;</td>\n",
       "      <td>3.830252</td>\n",
       "      <td>3.830252</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n",
       "      <td>&lt;pad&gt; Answer the question. The Cross Bakery building is on your left on Broadway.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Go straight down Broadway-Ellem and hang a left at the intersection of Broadway and Elm. Don't forget \"North Avenue\" to \"Urban Station\" in the middle of quellext one block. You'll find the Golden Gate Plaza Hotel at the very end of this installment! Every cinema department in the United States should have this cooling down system.&lt;/s&gt;</td>\n",
       "      <td>4.000775</td>\n",
       "      <td>4.000775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n",
       "      <td>&lt;pad&gt; People gatewayed into the house, chasing a criminal down. They didn't believe the burglar when they first walked in. They think somebody stole all their electronics. They need to go look upstairs.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; (Click_caption]) Then click a text to find it—if it had been opened. Click Or (ClickVerse) now for the closest convenience store. (Click a image to open S:the advertisement. Arranged sidewinder formula killer BATFINIs are a crime in their “slicing it”. about epilepsy 7 mineral powderders [...] to Wipe away toxins from their supports following Judas Simpson absconders &amp;ersinprene’, Jennifer Ellen Perry 5 Alleged Cheating in 8/20 zola.ch http://www.normalemotional.gc/asne.catee/#expected...</td>\n",
       "      <td>3.604394</td>\n",
       "      <td>3.604394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda is the new CEO/Clout representative. He sold HaseenCase Lux. He is pretty proficient with some details. As an executive he has mentioned a number of merchandise pieces that fit him well.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #PersonPnter Bother share the jokes of the past. #Person1 doesn't like Helen Walker famous cap. #Person2athe branduckin and others were looking at matching caps.&lt;/s&gt;</td>\n",
       "      <td>3.257032</td>\n",
       "      <td>3.257032</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n",
       "      <td>&lt;pad&gt; Work. Have fun tonight!&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; speaking to/out to person 135.&lt;/s&gt;</td>\n",
       "      <td>3.985719</td>\n",
       "      <td>3.985719</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Everybody is asking for a car toy and in this case I'd like a car. Who kindly asks for my help?&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Shop | Also in the effects of brakes, headphones, gel injection fingernails, nippers, elder pots, waifs And $2.125million Dog in the dust and humidity where out there The advisory is headlamp on base of the air brake featuress, crash amniion pads and a Blosport that reinforced almost nothing on top. The Steve Hurley Sr. joedge surround with the unbe so swaybilly Sean's helmet heat and style.&lt;/s&gt;</td>\n",
       "      <td>2.958119</td>\n",
       "      <td>2.958119</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n",
       "      <td>&lt;pad&gt; Alice can't go to see Mrs. Brown tomorrow morning because her mother is ill.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Message The TV program on Miss U.&lt;/s&gt;</td>\n",
       "      <td>3.820549</td>\n",
       "      <td>3.820549</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n",
       "      <td>&lt;pad&gt; We tell in this interview about various times when families have computers. In this interview we want to share some of our experiences with products that people have effectively used to make them easier to use.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; People are getting up and down with the People are showing online.&lt;/s&gt;</td>\n",
       "      <td>4.329280</td>\n",
       "      <td>4.329280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...</td>\n",
       "      <td>&lt;pad&gt; #Person1#: Jason. Situation is that we've headed to their home. It's a long walk and then it's too late to get to their rehearsal.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; She answers the guitar as a student. They begin planning a live concert March Mad Monday.&lt;/s&gt;</td>\n",
       "      <td>4.002971</td>\n",
       "      <td>4.002971</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n",
       "      <td>&lt;pad&gt; People are overworked by today's deadline for their follow-up report.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Here are some tips for a good one at an urgent meeting.&lt;/s&gt;</td>\n",
       "      <td>2.884351</td>\n",
       "      <td>2.884351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; The plane landed slowly. A piece of luggage broke in the break. The plane hasn't been clearing up any luggage.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; What's a lastpoen for you?&lt;/s&gt;</td>\n",
       "      <td>4.034144</td>\n",
       "      <td>4.034144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
       "0   Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n",
       "1                                                                                                                                                                   Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: </s>   \n",
       "2   Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n",
       "3   Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n",
       "4                                                                                     Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: </s>   \n",
       "5                                                                                                                                                                         Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: </s>   \n",
       "6   Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n",
       "7   Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n",
       "8   Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n",
       "9                       Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: </s>   \n",
       "10  Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n",
       "11  Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n",
       "12                                                                                                                                                                                                                          Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: </s>   \n",
       "13  Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n",
       "14          Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: </s>   \n",
       "15  Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n",
       "16  Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n",
       "17  Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh... Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You t...   \n",
       "18  Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n",
       "19                                                                                                                                                                                                                                        Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: </s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                  response_before  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                 <pad> Commute your cell phone to your home.</s>   \n",
       "1                                                                                                                                                                                                                                                                                                      <pad> Judy, please believe everyone. Richard was fired by our manager.</s>   \n",
       "2                                                                                                                                                                                                  <pad> Dallas C. Park's clinic is located in China's Town Center shopping mall. taxi (town limo) service is now provided, but you can make a local day-trip from Guangzhou.</s>   \n",
       "3                                                                                                                                                                                                                                                                                                                             <pad> To let you know what the express is like.</s>   \n",
       "4                                                                                                                                                                                                                                                                                                             <pad> People at the Maa * change buttons to cm movement in 10n.</s>   \n",
       "5                                                                                                                                                                                                                                                                                                                                                 <pad> Talk to Device dealer</s>   \n",
       "6                                                                                                                                                                                                                                                                                                               <pad> #Person1#: See the final draft of our contract (#1371).</s>   \n",
       "7                                                                                                                                                                                                                                                                                                                                          <pad> Find jobs in the job center.</s>   \n",
       "8   <pad> #Person1: Have you talked to your girlfriend this morning? She said, \"I am having trouble with alcoholamps.\" He is then annoyed for smoking in public. He says there are other ways, like nicotine patches or nicotine chewing gum. He is simply sorting through the smoke and this lady became nauseated. #Person1: Sorry honey too, honey. You're still smoking!!</s>   \n",
       "9                                                                                                                                                                                                                                                   <pad> Person1 needs a check to make sure she prepared good, original papers. They should be translated well into English.</s>   \n",
       "10                                                                                                                                                                                                                                                                                          <pad> Answer the question. The Cross Bakery building is on your left on Broadway.</s>   \n",
       "11                                                                                                                                                                 <pad> People gatewayed into the house, chasing a criminal down. They didn't believe the burglar when they first walked in. They think somebody stole all their electronics. They need to go look upstairs.</s>   \n",
       "12                                                                                                                                                                    <pad> Amanda is the new CEO/Clout representative. He sold HaseenCase Lux. He is pretty proficient with some details. As an executive he has mentioned a number of merchandise pieces that fit him well.</s>   \n",
       "13                                                                                                                                                                                                                                                                                                                                              <pad> Work. Have fun tonight!</s>   \n",
       "14                                                                                                                                                                                                                                                                      <pad> Everybody is asking for a car toy and in this case I'd like a car. Who kindly asks for my help?</s>   \n",
       "15                                                                                                                                                                                                                                                                                         <pad> Alice can't go to see Mrs. Brown tomorrow morning because her mother is ill.</s>   \n",
       "16                                                                                                                                                   <pad> We tell in this interview about various times when families have computers. In this interview we want to share some of our experiences with products that people have effectively used to make them easier to use.</s>   \n",
       "17                                                                                                                                                                                                                                   <pad> #Person1#: Jason. Situation is that we've headed to their home. It's a long walk and then it's too late to get to their rehearsal.</s>   \n",
       "18                                                                                                                                                                                                                                                                                                <pad> People are overworked by today's deadline for their follow-up report.</s>   \n",
       "19                                                                                                                                                                                                                                                       <pad> The plane landed slowly. A piece of luggage broke in the break. The plane hasn't been clearing up any luggage.</s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         response_after  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                      <pad> #Person1#: I need to buy some broadband latex. #Person2#: That's right</s>   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <pad> Judy and Classy about Judy's move.</s>   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                          <pad> and look. and sign in. and leave the prescription.</s>   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                        <pad> Assuming food quality was poor and the service wasn't good person2 says... this is a new restaurant.</s>   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                              <pad> The bill of 150 yuan a person is 1500 dollars.</s>   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <pad> Pict for $10 and saved at person1.</s>   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <pad> ---- My pleasure, Mr. Keith.</s>   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                       <pad> The characters are asked to pick a job. The job centers have hundreds of job job postings, all right?</s>   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <pad> The two people are talking about approval.</s>   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <pad> Hopefully it will be perfect soon.</s>   \n",
       "10                                                                                                                                                               <pad> Go straight down Broadway-Ellem and hang a left at the intersection of Broadway and Elm. Don't forget \"North Avenue\" to \"Urban Station\" in the middle of quellext one block. You'll find the Golden Gate Plaza Hotel at the very end of this installment! Every cinema department in the United States should have this cooling down system.</s>   \n",
       "11  <pad> (Click_caption]) Then click a text to find it—if it had been opened. Click Or (ClickVerse) now for the closest convenience store. (Click a image to open S:the advertisement. Arranged sidewinder formula killer BATFINIs are a crime in their “slicing it”. about epilepsy 7 mineral powderders [...] to Wipe away toxins from their supports following Judas Simpson absconders &ersinprene’, Jennifer Ellen Perry 5 Alleged Cheating in 8/20 zola.ch http://www.normalemotional.gc/asne.catee/#expected...   \n",
       "12                                                                                                                                                                                                                                                                                                                                          <pad> #PersonPnter Bother share the jokes of the past. #Person1 doesn't like Helen Walker famous cap. #Person2athe branduckin and others were looking at matching caps.</s>   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <pad> speaking to/out to person 135.</s>   \n",
       "14                                                                                                 <pad> Shop | Also in the effects of brakes, headphones, gel injection fingernails, nippers, elder pots, waifs And $2.125million Dog in the dust and humidity where out there The advisory is headlamp on base of the air brake featuress, crash amniion pads and a Blosport that reinforced almost nothing on top. The Steve Hurley Sr. joedge surround with the unbe so swaybilly Sean's helmet heat and style.</s>   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <pad> #Message The TV program on Miss U.</s>   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                         <pad> People are getting up and down with the People are showing online.</s>   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                  <pad> She answers the guitar as a student. They begin planning a live concert March Mad Monday.</s>   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                    <pad> Here are some tips for a good one at an urgent meeting.</s>   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <pad> What's a lastpoen for you?</s>   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        3.031390      3.031390          0.0  \n",
       "1        3.904954      3.904954          0.0  \n",
       "2        3.943122      3.943122          0.0  \n",
       "3        3.387951      3.387951          0.0  \n",
       "4        3.958113      3.958113          0.0  \n",
       "5        3.289397      3.289397          0.0  \n",
       "6        4.364748      4.364748          0.0  \n",
       "7        3.839757      3.839757          0.0  \n",
       "8        2.772885      2.772885          0.0  \n",
       "9        3.830252      3.830252          0.0  \n",
       "10       4.000775      4.000775          0.0  \n",
       "11       3.604394      3.604394          0.0  \n",
       "12       3.257032      3.257032          0.0  \n",
       "13       3.985719      3.985719          0.0  \n",
       "14       2.958119      2.958119          0.0  \n",
       "15       3.820549      3.820549          0.0  \n",
       "16       4.329280      4.329280          0.0  \n",
       "17       4.002971      4.002971          0.0  \n",
       "18       2.884351      2.884351          0.0  \n",
       "19       4.034144      4.034144          0.0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
