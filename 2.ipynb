{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef40aa26-9a75-42ca-886c-1da7e8530881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchdata transformers datasets evaluate rouge_score loralib peft --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed403d-2669-48ad-9bbb-9578c5f32a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4ab36-bc68-45e9-a908-849bace3b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "#from transformers import AutoModelForSeq2SeqLM, -- python library for transformers\n",
    "#AutoTokenizer, \n",
    "#GenerationConfig, \n",
    "#TrainingArguments -- ways that simplifies code when fine tuning lllm\n",
    "#Trainer -- ways that simplifies code when fine tuning lllm\n",
    "#import torch\n",
    "#import time\n",
    "#import evaluate\n",
    "#import pandas as pd\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336277db-f2db-4019-951b-8a39a3600586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aaca638-95ce-485d-b60f-f9e4e1d71bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/natalr2/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d639f458364c5f89afdb5e6dfd5837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_hf = \"knkarthick/dialogsum\"\n",
    "\n",
    "ds = load_dataset(ds_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dc8202-c7ef-4691-98b1-00df66eeaa99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5d6320-b207-4617-a7d9-66b3455f048d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the model using Flan t5 -- flan t5 is a general model that can do wide variety of tasks\n",
    "\n",
    "model_name= 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16) #torch_dtype=torch.bfloat16 specify the memory type to be used in this model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33cd063a-9b45-4f3d-949e-f6be643d1001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856 \n",
      "percentage of trainable model parameters: (247577856/247577856)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params} \\npercentage of trainable model parameters: ({all_model_params}/{trainable_model_params})\\n\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60785638-3f1c-4960-86b4-f61ae7a81bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation. \\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation = True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding = \"max_length\", truncation = True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "    \n",
    "# dataset contains train, validation, test\n",
    "# tokenize_function is handling all data accross splits in batches\n",
    "tokenized_datasets = ds.map(tokenize_function, batched= True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id','topic','dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9040dc91-67d9-465f-9c7d-a0026cd5d83e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices = True) #subsampling to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62d560cc-efd7-4bd8-83ad-2c5ef57d4a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 2)\n",
      "(5, 2)\n",
      "(15, 2)\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'].shape)\n",
    "print(tokenized_datasets['validation'].shape)\n",
    "print(tokenized_datasets['test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6281cd97-ef77-47cc-be5d-86d0088f2727",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b5e71b-8aa2-47af-b146-19eae6ac5f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utilizing the hf TRainer class, passing the pre processed dataset with reference to the original model, adding some training parameters experimentally\n",
    "\n",
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1, #increase the number\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1 #increase size\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b6e9988-b975-45de-bcd1-72e9b4557c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=47.5, metrics={'train_runtime': 175.8005, 'train_samples_per_second': 0.046, 'train_steps_per_second': 0.006, 'total_flos': 5478058819584.0, 'train_loss': 47.5, 'epoch': 0.06})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa052d-de07-446a-9b65-e29c44e4f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download checkpoint of fully fine tuned model to use in the rest of the note book\n",
    "#!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40f474-2313-4857-8a7d-052e92ad43be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84f5f658-2fe1-4640-9294-a963f8671497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"/dialogue-summary-training-1702507973\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecdc8188-0a1e-4ff6-ae49-d6aec9950965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a instance of the Automodelfor seq2seqlmclass for the instrument model:\n",
    "\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"/dialogue-summary-training-1702507973/\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0670b54a-a582-400d-bad4-5303ab4cdff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL: \n",
      "#Person1#: I'm not sure what exactly I would need to upgrade my software.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT PROMPT:\n",
      "#Person1: I'm thinking of upgrading your system.\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model quantitativelly -- ROUGE\n",
    "\n",
    "index = 200\n",
    "dialogue = ds['test'][index]['dialogue']\n",
    "human_baseline_summary = ds['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))                                               \n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "                                                 \n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)                                                \n",
    "\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "                                                 \n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY: \\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL: \\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT PROMPT:\\n{instruct_model_text_output}')                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "560e02b5-c25a-4cb8-930e-311c64e15631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74833d3150c4b0dbfa426d13e348ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load rouge\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd67ea2a-c607-4408-90d4-c0a6a72057fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dialogues = ds['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = ds['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    \n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = [\"human_baseline_summaries\", \"original_model_summaries\", \"instruct_model_summaries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bfc4055-aa74-46e7-a9b2-4f74cb8b0a54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.21948246198246202, 'rouge2': 0.06779380040249605, 'rougeL': 0.18491961741961746, 'rougeLsum': 0.18630221630221633}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.24089921652421653, 'rouge2': 0.11769053708439897, 'rougeL': 0.22001958689458687, 'rougeLsum': 0.22134175465057818}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions = original_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True,\n",
    ")\n",
    "\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e440bf47-3d35-4ec0-b18e-9c53cd2001e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PEFT/lora for fine\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, #rank -- high\n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM #FLAN-T5\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5ea8552-afad-4a13-b180-fe426ca42283",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800 \n",
      "percentage of trainable model parameters: (251116800/3538944)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f73dc4a2-d0f8-45cd-b3dc-2493db7a0835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size = True,\n",
    "    learning_rate=1e-5, #higher than full fine-tuning\n",
    "    num_train_epochs=1, #increase the number\n",
    "    logging_steps=1,\n",
    "    max_steps=1 #increase size\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args = peft_training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0cb60cf-d6dc-4518-81b5-ee2e399261da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local\\\\tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local\\\\special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34ae9332-55d5-47f0-b191-96856894829d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       \"./peft-dialogue-summary-checkpoint-local/\",\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False #setting to false will tell torch that I'm not interested in train the model, just get the model -- basically minimize the footprint\n",
    "                                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4fe6051-b101-42fb-8148-c44d5834b2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800 \n",
      "percentage of trainable model parameters: (251116800/0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23632452-e9aa-41b1-998b-4c01d4b13417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL: \n",
      "#Person1: I'd like to upgrade my computer hardware. #Person1: I'm not sure what I'd need. #Person1: I'd like to upgrade my computer. #Person1: I'd like to make my own flyers and banners. #Person2: I'd like to add a CD-ROM drive. #Person1: I'm not sure what I'd need. #Person2: I'd like to add a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT PROMPT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT PROMPT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model quantitativelly -- ROUGE\n",
    "\n",
    "index = 200\n",
    "dialogue = ds['test'][index]['dialogue']\n",
    "human_baseline_summary = ds['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))                                               \n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "                                                 \n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)  \n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "                                                 \n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY: \\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL: \\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT PROMPT:\\n{instruct_model_text_output}')      \n",
    "print(dash_line)\n",
    "print(f'PEFT PROMPT:\\n{peft_model_text_output}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "355e035c-a4b2-45a1-87d8-a146fcc9f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = ds['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = ds['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    \n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = [\"human_baseline_summaries\", \"original_model_summaries\", \"instruct_model_summaries\", \"peft_model_summaries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74fe768e-62fb-4f69-b738-186e211db58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2657862258847701, 'rouge2': 0.11158190601668863, 'rougeL': 0.22359027281627897, 'rougeLsum': 0.2275186552353735}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.24089921652421653, 'rouge2': 0.11769053708439897, 'rougeL': 0.22001958689458687, 'rougeLsum': 0.22134175465057818}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.241950545026632, 'rouge2': 0.1179539641943734, 'rougeL': 0.22166387959866218, 'rougeLsum': 0.22283940294809862}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions = original_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions = instruct_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True,\n",
    ")\n",
    "\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions = peft_model_summaries,\n",
    "    references = human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator = True,\n",
    "    use_stemmer = True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results) #despite the similar results, PEFT uses way less resources in a big scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
